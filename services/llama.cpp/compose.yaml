services:
  llama-cpp:
    labels:
      - traefik.enable=true
      - traefik.http.routers.llama.rule=Host(`llama.gryta.eu`)
      - traefik.http.routers.llama.entrypoints=websecure
      - traefik.http.routers.llama.tls.certresolver=le
      - traefik.http.services.llama.loadbalancer.server.port=8000
    image: ghcr.io/ggml-org/llama.cpp:server-intel
    container_name: llama-cpp
    devices:
      - /dev/dri:/dev/dri
    volumes:
      - models:/models
    environment:
      LLAMA_ARG_MODEL: /models/Qwen2.5-Coder-3B-Instruct-Q4_K_M.gguf
      LLAMA_ARG_HOST: 0.0.0.0
      LLAMA_ARG_CTX_SIZE: 45096
      LLAMA_ARG_N_PARALLEL: 1
      LLAMA_ARG_ENDPOINT_METRICS: 1
      LLAMA_ARG_PORT: 8000
      SYCL_CACHE_PERSISTENT: 1
      SYCL_UR_TRACE: 1
      # API
      LLAMA_API_KEY: ${LLAMA_API_KEY}
      LLAMA_ARG_JINJA: 1
      LLAMA_LOG_VERBOSITY: 4
    healthcheck:
      test:
        - CMD
        - curl
        - -f
        - http://localhost:8000/health
      interval: 1m
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      homelab-network:
        ipv4_address: 172.20.0.15
    restart: unless-stopped
volumes:
  models: null
networks:
  homelab-network:
    external: true
